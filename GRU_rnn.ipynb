{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUqQgFrc07sTdGojZCaGEe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sames12421/RNN-PROJECT/blob/main/GRU_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "lfaEqswwuRC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=['i love adi','adi loves me','anupriya is loyal friend of mine']"
      ],
      "metadata": {
        "id": "GOVhTa2wue_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "sequential=tokenizer.texts_to_sequences(text)"
      ],
      "metadata": {
        "id": "xGQgqbxNus3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_lenght=max([len(seq) for seq in sequential]) # Changed 'sequencial' to 'sequential'\n",
        "padding_sequence=pad_sequences(sequential,maxlen=max_lenght,padding='post') # Changed 'pd_sequences' to 'pad_sequences'"
      ],
      "metadata": {
        "id": "to93UvRiwApi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1,output_dim=10,input_length=5),\n",
        "    GRU(50,activation='relu',return_sequences=True,input_shape=(10,1)),\n",
        "    GRU(50,activation='relu'),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r0WFMlSu9Kz",
        "outputId": "3da5649c-fe0d-4c84-8a00-92cf7f154fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "76u0OcmuvT7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt: summary of the above code\n",
        "\n",
        "This code snippet creates and compiles a simple recurrent neural network (RNN) using TensorFlow/Keras for text processing.  Let's break down the key parts:\n",
        "\n",
        "1. **Data Preparation:**\n",
        "   - A list of three short sentences is defined as the input text data.\n",
        "   - A `Tokenizer` is used to convert the text into numerical sequences (tokens).  Each unique word is assigned a numerical index.\n",
        "   - `texts_to_sequences` transforms the text sentences into lists of these numerical indices.\n",
        "   - `pad_sequences` ensures all sequences have the same length by padding shorter sequences with zeros.\n",
        "\n",
        "2. **Model Building:**\n",
        "   - A sequential model is created.\n",
        "   - An `Embedding` layer transforms the integer sequences into dense vectors of fixed size (10 in this case). This layer learns word representations.\n",
        "   - Two `GRU` (Gated Recurrent Unit) layers are added, which are a type of RNN designed to capture temporal dependencies in sequential data.  The first GRU returns sequences, while the second returns a single vector. `relu` activation is used.\n",
        "   - A `Dense` layer with a single output neuron completes the model.\n",
        "\n",
        "3. **Model Compilation:**\n",
        "   - The model is compiled with the 'adam' optimizer, 'categorical_crossentropy' loss function (though this is unusual for a single output neuron; binary cross-entropy or mean squared error would likely be more appropriate), and 'accuracy' as the evaluation metric.\n",
        "\n",
        "**Potential Issues and Improvements:**\n",
        "\n",
        "* **Loss Function:** The `categorical_crossentropy` loss function is likely incorrect given the single output neuron in the final Dense layer.  Binary cross-entropy or mean squared error should be used depending on the intended task (e.g., binary classification or regression).\n",
        "* **Input Shape:** The `input_shape` argument in the first GRU layer seems incorrect.  The input shape should match the output of the embedding layer, but here `input_length` is set to 5, which might not be correct.\n",
        "* **Target Variable:** The code is missing the target variable or labels for training.  To train the model, you need to specify the correct output for each input sentence.\n",
        "* **Max Length:** The `max_lenght` variable is used in the padding, but it doesn't dynamically determine the padding length. It should be adjusted based on the data.\n",
        "* **Output activation:**  The output layer might need an activation function depending on what this model will be doing (e.g. softmax for multi-class classification, sigmoid for binary classification).\n",
        "\n",
        "\n",
        "This summary provides a high-level overview of the code's purpose and identifies some critical areas for improvement.  The code, in its current state, is incomplete and requires further adjustments to be usable.\n"
      ],
      "metadata": {
        "id": "9jesRPtP0YIV"
      }
    }
  ]
}